{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, x, y, t, u, v, layers):\n",
    "\n",
    "        X = np.concatenate([x, y, t], 1)\n",
    "\n",
    "        self.lb = X.min(0)\n",
    "        self.ub = X.max(0)\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        self.x = tf.convert_to_tensor(X[:, 0:1], dtype=tf.float32)\n",
    "        self.y = tf.convert_to_tensor(X[:, 1:2], dtype=tf.float32)\n",
    "        self.t = tf.convert_to_tensor(X[:, 2:3], dtype=tf.float32)\n",
    "\n",
    "        self.u = tf.convert_to_tensor(u, dtype=tf.float32)\n",
    "        self.v = tf.convert_to_tensor(v, dtype=tf.float32)\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        # Initialize NN\n",
    "        self.model = self.create_model(layers)\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
    "        self.lambda_2 = tf.Variable([0.0], dtype=tf.float32)\n",
    "\n",
    "    def create_model(self, layers):\n",
    "        model = tf.keras.Sequential()\n",
    "        for i in range(len(layers)-2):\n",
    "            model.add(tf.keras.layers.Dense(layers[i+1], activation=tf.nn.tanh,\n",
    "                                            input_shape=(layers[i],),\n",
    "                                            kernel_initializer='glorot_normal'))\n",
    "        model.add(tf.keras.layers.Dense(layers[-1], kernel_initializer='glorot_normal'))\n",
    "        return model\n",
    "\n",
    "    def neural_net(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "    def loss(self, u_pred, v_pred, f_u_pred, f_v_pred):\n",
    "        return tf.reduce_sum(tf.square(self.u - u_pred)) + \\\n",
    "               tf.reduce_sum(tf.square(self.v - v_pred)) + \\\n",
    "               tf.reduce_sum(tf.square(f_u_pred)) + \\\n",
    "               tf.reduce_sum(tf.square(f_v_pred))\n",
    "\n",
    "    def net_NS(self, x, y, t):\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch([x, y, t])\n",
    "            with tf.GradientTape(persistent=True) as tape1:\n",
    "                tape1.watch([x, y, t])\n",
    "                psi_and_p = self.neural_net(tf.concat([x, y, t], 1))\n",
    "                psi = psi_and_p[:, 0:1]\n",
    "                p = psi_and_p[:, 1:2]\n",
    "                u = tape1.gradient(psi, t)\n",
    "                v = -tape1.gradient(psi, x)\n",
    "            u_x = tape1.gradient(u, x)\n",
    "            u_y = tape1.gradient(u, y)\n",
    "            v_x = tape1.gradient(v, x)\n",
    "            v_y = tape1.gradient(v, y)\n",
    "            u_t = tape2.gradient(u, t)\n",
    "            v_t = tape2.gradient(v, t)\n",
    "            u_xx = tape2.gradient(u_x, x)\n",
    "            u_yy = tape2.gradient(u_y, y)\n",
    "            v_xx = tape2.gradient(v_x, x)\n",
    "            v_yy = tape2.gradient(v_y, y)\n",
    "        p_x = tape2.gradient(p, x)\n",
    "        p_y = tape2.gradient(p, y)\n",
    "\n",
    "        f_u = u_t + lambda_1 * (u * u_x + v * u_y) + p_x - lambda_2 * (u_xx + u_yy)\n",
    "        f_v = v_t + lambda_1 * (u * v_x + v * v_y) + p_y - lambda_2 * (v_xx + v_yy)\n",
    "\n",
    "        return u, v, p, f_u, f_v\n",
    "\n",
    "\n",
    "\n",
    "    def callback(self, loss, lambda_1, lambda_2):\n",
    "        print('Loss: %.3e, l1: %.3f, l2: %.5f' % (loss, lambda_1, lambda_2))\n",
    "\n",
    "    # def train(self, nIter):\n",
    "    #     optimizer = Adam()\n",
    "\n",
    "    #     for it in range(nIter):\n",
    "    #         with tf.GradientTape() as tape:\n",
    "    #             u_pred, v_pred, p_pred, f_u_pred, f_v_pred = self.net_NS(self.x, self.y, self.t)\n",
    "    #             loss_value = self.loss(u_pred, v_pred, f_u_pred, f_v_pred)\n",
    "\n",
    "    #         grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "    #         optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "    #         # Print\n",
    "    #         if it % 10 == 0:\n",
    "    #             lambda_1_value = self.lambda_1.numpy()\n",
    "    #             lambda_2_value = self.lambda_2.numpy()\n",
    "    #             print('It: %d, Loss: %.3e, l1: %.3f, l2: %.5f' %\n",
    "    #                   (it, loss_value, lambda_1_value, lambda_2_value))\n",
    "\n",
    "    #     # SciPy optimization after Adam optimization\n",
    "    #     method = 'L-BFGS-B'\n",
    "    #     options = {'maxiter': 50000,\n",
    "    #                'maxfun': 50000,\n",
    "    #                'maxcor': 50,\n",
    "    #                'maxls': 50,\n",
    "    #                'ftol': 1.0 * np.finfo(float).eps}\n",
    "\n",
    "    #     def loss_function(variables):\n",
    "    #         self.lambda_1.assign([variables[0]])\n",
    "    #         self.lambda_2.assign([variables[1]])\n",
    "    #         u_pred, v_pred, p_pred, f_u_pred, f_v_pred = self.net_NS(self.x, self.y, self.t)\n",
    "    #         loss_value = self.loss(u_pred, v_pred, f_u_pred, f_v_pred)\n",
    "    #         return loss_value.numpy()\n",
    "\n",
    "    #     def loss_gradient(variables):\n",
    "    #         with tf.GradientTape() as tape:\n",
    "    #             loss_value = loss_function(variables)\n",
    "    #         grad = tape.gradient(loss_value, [self.lambda_1, self.lambda_2])\n",
    "    #         return tf.convert_to_tensor([grad[0], grad[1]])\n",
    "\n",
    "    #     variables = tf.Variable([0.0, 0.0], dtype=tf.float32)\n",
    "    #     result = minimize(loss_function, variables, method=method, jac=loss_gradient, options=options)\n",
    "    #     self.lambda_1.assign([result.x[0]])\n",
    "    #     self.lambda_2.assign([result.x[1]])\n",
    "\n",
    "\n",
    "    def train(self, nIter):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        for it in range(nIter):\n",
    "            with tf.GradientTape() as tape:\n",
    "                u_pred, v_pred, p_pred, f_u_pred, f_v_pred = self.net_NS(self.x, self.y, self.t)\n",
    "                loss_value = self.loss(u_pred, v_pred, f_u_pred, f_v_pred)\n",
    "\n",
    "            gradients = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "\n",
    "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                print(\"Iteration: {}, Loss: {:.4e}\".format(it, loss_value.numpy()))\n",
    "\n",
    "        self.lambda_1 = self.model.variables[0].numpy()\n",
    "        self.lambda_2 = self.model.variables[1].numpy()\n",
    "\n",
    "\n",
    "    def predict(self, x_star, y_star, t_star):\n",
    "\n",
    "        u_star, v_star, p_star, _, _ = self.net_NS(x_star, y_star, t_star)\n",
    "\n",
    "        return u_star, v_star, p_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 5000\n",
    "    \n",
    "layers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 2]\n",
    "\n",
    "# Load Data\n",
    "data = scipy.io.loadmat('./PINNs-master/main/Data/cylinder_nektar_wake.mat')\n",
    "        \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None] # NT x 1\n",
    "y = YY.flatten()[:,None] # NT x 1\n",
    "t = TT.flatten()[:,None] # NT x 1\n",
    "\n",
    "u = UU.flatten()[:,None] # NT x 1\n",
    "v = VV.flatten()[:,None] # NT x 1\n",
    "p = PP.flatten()[:,None] # NT x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(N*T, N_train, replace=False)\n",
    "x_train = x[idx,:]\n",
    "y_train = y[idx,:]\n",
    "t_train = t[idx,:]\n",
    "u_train = u[idx,:]\n",
    "v_train = v[idx,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhysicsInformedNN(x_train, y_train, t_train, u_train, v_train, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "Iteration: 0, Loss: 4.4366e+03\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    }
   ],
   "source": [
    "model.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "snap = np.array([100])\n",
    "x_star = X_star[:,0:1]\n",
    "y_star = X_star[:,1:2]\n",
    "t_star = TT[:,snap]\n",
    "\n",
    "u_star = U_star[:,0,snap]\n",
    "v_star = U_star[:,1,snap]\n",
    "p_star = P_star[:,snap]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
