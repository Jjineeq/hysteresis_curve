{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 정의 (여기서는 학습 데이터를 임의로 정의합니다. 실제로는 데이터를 제공해야 합니다.)\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.linspace(0, 1, 100)\n",
    "t = np.linspace(0, 1, 100)\n",
    "\n",
    "X, Y, T = np.meshgrid(x, y, t)\n",
    "\n",
    "# 학습 데이터 구조 변경\n",
    "train_data = np.vstack((X.flatten(), Y.flatten(), T.flatten())).T\n",
    "train_data = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "\n",
    "# 2. 신경망 정의\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),  # x, y, t를 입력으로 받습니다.\n",
    "    tf.keras.layers.Dense(20, activation='LeakyReLU'),\n",
    "    tf.keras.layers.Dense(20, activation='LeakyReLU'),\n",
    "    tf.keras.layers.Dense(1)  # u의 값을 출력합니다.\n",
    "])\n",
    "\n",
    "def pde_loss(model, x):\n",
    "    with tf.GradientTape(persistent=True) as outer_tape:\n",
    "        outer_tape.watch(x)\n",
    "        with tf.GradientTape(persistent=True) as inner_tape:\n",
    "            inner_tape.watch(x)\n",
    "            u = model(x)\n",
    "        \n",
    "        u_t = outer_tape.gradient(u, x)[:, 0]  # u를 t에 대해서 미분\n",
    "        u_x = inner_tape.gradient(u, x)[:, 1]  # u를 x에 대해서 미분\n",
    "        u_y = inner_tape.gradient(u, x)[:, 2]  # u를 y에 대해서 미분\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as second_tape:\n",
    "        second_tape.watch(x)\n",
    "        u_x = second_tape.gradient(u, x)[:, 1]\n",
    "        u_y = second_tape.gradient(u, x)[:, 2]\n",
    "\n",
    "    u_xx = second_tape.gradient(u_x, x)[:, 1]\n",
    "    u_yy = second_tape.gradient(u_y, x)[:, 2]\n",
    "\n",
    "    del inner_tape\n",
    "    del outer_tape\n",
    "    del second_tape\n",
    "\n",
    "    D = 0.1  # 예를 들어 확산 계수를 0.1로 가정\n",
    "    f = u_t - D * (u_xx + u_yy)  # 미분 방정식의 차이\n",
    "\n",
    "    return tf.reduce_mean(tf.square(f))\n",
    "\n",
    "# 4. 학습\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# 학습 과정 정의\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = pde_loss(model, train_data)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> 4\u001b[0m     loss \u001b[39m=\u001b[39m train_step()\n\u001b[0;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m500\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mnumpy()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 59\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m():\n\u001b[0;32m     58\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> 59\u001b[0m         loss \u001b[39m=\u001b[39m pde_loss(model, train_data)\n\u001b[0;32m     61\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, model\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m     62\u001b[0m     optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, model\u001b[39m.\u001b[39mtrainable_variables))\n",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m, in \u001b[0;36mpde_loss\u001b[1;34m(model, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape(persistent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m second_tape:\n\u001b[0;32m     35\u001b[0m     second_tape\u001b[39m.\u001b[39mwatch(x)\n\u001b[1;32m---> 36\u001b[0m     u_x \u001b[39m=\u001b[39m second_tape\u001b[39m.\u001b[39;49mgradient(u, x)[:, \u001b[39m1\u001b[39;49m]\n\u001b[0;32m     37\u001b[0m     u_y \u001b[39m=\u001b[39m second_tape\u001b[39m.\u001b[39mgradient(u, x)[:, \u001b[39m2\u001b[39m]\n\u001b[0;32m     39\u001b[0m u_xx \u001b[39m=\u001b[39m second_tape\u001b[39m.\u001b[39mgradient(u_x, x)[:, \u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step()\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# 이렇게 학습을 진행한 후에는 model을 사용하여 (x, y, t) 위치에서의 u 값을 예측할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
